{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYDkIhdN0y9"
      },
      "source": [
        "In this notebook we implement three different methods for computing optimal factorizations for [the matrix mechanism](https://people.cs.umass.edu/~mcgregor/papers/15-vldbj.pdf) under approximate differential privacy:\n",
        "\n",
        "1. Gradient descent on an associated convex problem\n",
        "1. The fixed-point iteration method described in the paper associated to this code.\n",
        "1. A Newton-direction-based algorithm designed in [previous literature](https://arxiv.org/pdf/1602.04302v1.pdf).\n",
        "\n",
        "We experimentally compare their numerical efficiency on the problem of factorizing the prefix-sum matrix: the lower-triangular matrix of all 1s, which takes a vector to its vector of partial sums. Other matrices can be used, and will generally produce similar results.\n",
        "\n",
        "One note on initialization below: it is difficult to initialize the two descent-based schemes and the fixed-point based algorithm identically. In order to ensure similar initialization, it is easiest to select a *vector* (corresponding to the parameterization of the fixed-point algorithm), and attempt to generate a matrix from this vector. Generating this matrix, however, essentially takes advantage of the representations which yield the fixed-point problem--and this generated matrix usually has significantly lower loss than a general positive definite matrix with constant 1s on the diagonal. This observation is not necessarily surprising; the dimensionalities involved in a vector parameterization are much lower. It is slightly unfair to the fixed-point method to 'allow' the gradient-based methods to use this initialization; but since the fixed-point method is the one we propose, we reserve the right to make it look slightly worse than it otherwise might."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J1Z1q8omN1sc"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "import time\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import value_and_grad, jit\n",
        "from jax import config\n",
        "import numpy as np\n",
        "\n",
        "# With large matrices, the extra precision afforded by performing all\n",
        "# computations in float64 is critical.\n",
        "config.update('jax_enable_x64', True)\n",
        "\n",
        "\n",
        "@jit\n",
        "def diagonalize_and_take_jax_matrix_sqrt(matrix: jnp.ndarray, min_eval: float = 0.0) -> jnp.ndarray:\n",
        "  \"\"\"Matrix square root for positive-semi-definite, Hermitian matrices.\"\"\"\n",
        "  evals, evecs = jnp.linalg.eigh(matrix)\n",
        "  eval_sqrt = jnp.maximum(evals, min_eval)**0.5\n",
        "  sqrt = evecs @ jnp.diag(eval_sqrt) @ evecs.T\n",
        "  return sqrt\n",
        "\n",
        "def hermitian_adjoint(matrix: jnp.ndarray) -> jnp.ndarray:\n",
        "  return jnp.conjugate(matrix).T\n",
        "\n",
        "def compute_loss_in_x(target: jnp.ndarray, x: jnp.ndarray):\n",
        "  m = hermitian_adjoint(target) @ target @ jnp.linalg.inv(x)\n",
        "  raw_trace = jnp.trace(m)\n",
        "  max_diag = jnp.max(jnp.diag(x))\n",
        "  return raw_trace * max_diag\n",
        "\n",
        "def compute_normalized_x_from_vector(matrix_to_factorize, v, precomputed_sqrt: Optional[jnp.ndarray] = None):\n",
        "  \"\"\"Computes a normalized (to all-1s diagonal) version of the vector -> matrix\n",
        "  mapping which defines the relationship between fixed points and optima.\n",
        "\n",
        "  At a fixed point of phi (equivalently an optimum of the symmetrized\n",
        "  factorization problem), the normalization below will no-op. But to normalize\n",
        "  between iterations of the fixed-point method and iterations of the descent-\n",
        "  based methods, it is useful to force the results of this transformation\n",
        "  to always have constant-1 diagonals.\n",
        "  \"\"\"\n",
        "  inv_diag_sqrt = jnp.diag(v ** -(0.5))\n",
        "  diag_sqrt = jnp.diag(v ** 0.5)\n",
        "  if precomputed_sqrt is None:\n",
        "    target = hermitian_adjoint(matrix_to_factorize) @ matrix_to_factorize\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(\n",
        "        diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt)\n",
        "  else:\n",
        "    # We simply assume that our caller did this computation correctly.\n",
        "    matrix_sqrt = precomputed_sqrt\n",
        "  x = inv_diag_sqrt @ matrix_sqrt @ inv_diag_sqrt\n",
        "  # Force all-1s on diagonal. This normalization is a requirement for the\n",
        "  # initial iterates of the descent-based methods, and we know it's true at the\n",
        "  # optimum.\n",
        "  x_sqrt_diag = jnp.diag(x)\n",
        "  normalized_x = jnp.diag((x_sqrt_diag)**(-0.5)) @ x @ jnp.diag((x_sqrt_diag)**(-0.5))\n",
        "  return normalized_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06vyKu8DqKN"
      },
      "source": [
        "# Algorithm implementation: gradient descent on the convex problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0GonuoXsN6gP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time in loop: [0.0, 0.015416145324707031, 0.028877735137939453, 0.04267454147338867, 0.05576348304748535, 0.06912374496459961, 0.08205819129943848, 0.09570908546447754, 0.10875296592712402, 0.12206411361694336, 0.13573575019836426, 0.14942336082458496, 0.16273164749145508, 0.17592597007751465, 0.18972444534301758, 0.20364713668823242, 0.21738338470458984, 0.23139715194702148, 0.24491143226623535, 0.2585721015930176, 0.2722022533416748, 0.28922128677368164, 0.3029634952545166, 0.31585240364074707, 0.3284142017364502, 0.34122633934020996, 0.391843318939209, 0.4208242893218994, 0.43453550338745117, 0.45150113105773926, 0.4682881832122803, 0.48172855377197266, 0.4984571933746338, 0.5352053642272949, 0.5552847385406494, 0.5689034461975098, 0.5824544429779053, 0.5961308479309082, 0.6096656322479248, 0.6237199306488037, 0.6372284889221191, 0.6508080959320068, 0.6643614768981934, 0.6778585910797119, 0.6914794445037842, 0.7050948143005371, 0.7186377048492432, 0.7326686382293701, 0.7455248832702637, 0.760453462600708, 0.7729456424713135, 0.7891476154327393, 0.8020806312561035, 0.8178658485412598, 0.8305912017822266, 0.8461732864379883, 0.8586430549621582, 0.874004602432251, 0.8865747451782227, 0.9023637771606445, 0.9152312278747559, 0.9309883117675781, 0.9440486431121826, 0.9599192142486572, 0.9731845855712891, 0.989208459854126, 1.002279281616211, 1.0628163814544678, 1.0832414627075195, 1.1002681255340576, 1.113755464553833, 1.1304550170898438, 1.1439809799194336, 1.1610851287841797, 1.1745519638061523, 1.1912145614624023, 1.2046597003936768, 1.221346378326416, 1.2348690032958984, 1.2521193027496338, 1.2656517028808594, 1.2823984622955322, 1.295893669128418, 1.3094470500946045, 1.3229761123657227, 1.3365304470062256, 1.350379467010498, 1.3700592517852783, 1.3846385478973389, 1.3980660438537598, 1.411670207977295, 1.4249622821807861, 1.438657522201538, 1.4520316123962402, 1.4655787944793701, 1.4790153503417969, 1.4925603866577148, 1.5059678554534912, 1.5194940567016602, 1.5329031944274902]\n",
            "[[1.         0.82342968 0.79046125 ... 0.0068329  0.00455158 0.00227372]\n",
            " [0.82342968 1.         0.81142944 ... 0.00679745 0.00452663 0.00226022]\n",
            " [0.79046125 0.81142944 1.         ... 0.00677246 0.00450926 0.00225104]\n",
            " ...\n",
            " [0.0068329  0.00679745 0.00677246 ... 1.         0.42517039 0.1849818 ]\n",
            " [0.00455158 0.00452663 0.00450926 ... 0.42517039 1.         0.325569  ]\n",
            " [0.00227372 0.00226022 0.00225104 ... 0.1849818  0.325569   1.        ]]\n",
            "[Array(8256., dtype=float64), Array(2104.27898517, dtype=float64), Array(1463.20285549, dtype=float64), Array(1342.11620743, dtype=float64), Array(898.68442588, dtype=float64), Array(854.2611167, dtype=float64), Array(845.62003445, dtype=float64), Array(836.9059374, dtype=float64), Array(829.75063811, dtype=float64), Array(823.42569101, dtype=float64), Array(817.63257377, dtype=float64), Array(772.81886588, dtype=float64), Array(769.88399766, dtype=float64), Array(766.58666863, dtype=float64), Array(751.34849153, dtype=float64), Array(745.69758548, dtype=float64), Array(743.87093445, dtype=float64), Array(741.54531554, dtype=float64), Array(739.79355655, dtype=float64), Array(729.77809885, dtype=float64), Array(732.98700254, dtype=float64), Array(732.75470826, dtype=float64), Array(729.96054992, dtype=float64), Array(726.50141681, dtype=float64), Array(724.52076941, dtype=float64), Array(723.14115161, dtype=float64), Array(722.01326771, dtype=float64), Array(721.04935302, dtype=float64), Array(717.76510309, dtype=float64), Array(713.92043327, dtype=float64), Array(713.03204186, dtype=float64), Array(712.39761558, dtype=float64), Array(710.95644731, dtype=float64), Array(707.74563308, dtype=float64), Array(706.93471389, dtype=float64), Array(706.32374253, dtype=float64), Array(704.79689553, dtype=float64), Array(710.26297197, dtype=float64), Array(705.50123981, dtype=float64), Array(702.66799677, dtype=float64), Array(701.7733758, dtype=float64), Array(701.28171963, dtype=float64), Array(700.9265133, dtype=float64), Array(698.9218984, dtype=float64), Array(698.36420357, dtype=float64), Array(697.80245687, dtype=float64), Array(697.54844471, dtype=float64), Array(697.30150078, dtype=float64), Array(697.08893183, dtype=float64), Array(696.86422506, dtype=float64), Array(696.65945896, dtype=float64), Array(696.44814742, dtype=float64), Array(696.25088281, dtype=float64), Array(696.05008644, dtype=float64), Array(695.86011613, dtype=float64), Array(695.66826187, dtype=float64), Array(695.48532825, dtype=float64), Array(695.30143273, dtype=float64), Array(695.12526672, dtype=float64), Array(694.94864397, dtype=float64), Array(694.77898377, dtype=float64), Array(694.60911191, dtype=float64), Array(694.44571142, dtype=float64), Array(694.28216622, dtype=float64), Array(694.12479964, dtype=float64), Array(693.96721877, dtype=float64), Array(693.81568406, dtype=float64), Array(693.66374636, dtype=float64), Array(693.51786896, dtype=float64), Array(693.37128107, dtype=float64), Array(693.23091864, dtype=float64), Array(693.08940518, dtype=float64), Array(692.95445389, dtype=float64), Array(692.81774879, dtype=float64), Array(692.68815172, dtype=float64), Array(692.55598904, dtype=float64), Array(692.4317471, dtype=float64), Array(692.30384972, dtype=float64), Array(692.18503518, dtype=float64), Array(692.06109986, dtype=float64), Array(691.94787182, dtype=float64), Array(691.82754905, dtype=float64), Array(691.72016907, dtype=float64), Array(691.60303679, dtype=float64), Array(691.50188011, dtype=float64), Array(691.38741202, dtype=float64), Array(691.29296712, dtype=float64), Array(691.18050008, dtype=float64), Array(691.09334568, dtype=float64), Array(690.98205682, dtype=float64), Array(690.90280545, dtype=float64), Array(690.79171673, dtype=float64), Array(690.72092023, dtype=float64), Array(690.60895157, dtype=float64), Array(690.54698115, dtype=float64), Array(690.43306445, dtype=float64), Array(690.38000101, dtype=float64), Array(690.26324101, dtype=float64), Array(690.21882562, dtype=float64), Array(690.09865831, dtype=float64)]\n",
            "0.1579340745210084\n"
          ]
        }
      ],
      "source": [
        "def optimize_factorization_grad_descent(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, lr: float = 1., use_armijo_rule: bool = True):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate(x_iter, grad, init_lr):\n",
        "    candidate = x_iter - grad * init_lr\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "    else:\n",
        "      sufficient_decrease_condition = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad ** 2)\n",
        "      if compiled_loss(candidate) <= sufficient_decrease_condition:\n",
        "        return candidate\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compute_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    diag_elements = jnp.diag_indices_from(grad)\n",
        "    grad1 = grad.at[diag_elements].set(0)\n",
        "    loss_array.append(loss)\n",
        "    if use_armijo_rule:\n",
        "      x_iter = find_next_iterate(x_iter, grad1, lr)\n",
        "    else:\n",
        "      x_iter = x_iter - lr * grad1\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2\n",
        "    \n",
        "    time_array.append(time.time() - start)\n",
        "  \n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "s_matrix = jnp.tril(jnp.ones(shape=(128, 128)))\n",
        "opt, losses, time_in_loop = optimize_factorization_grad_descent(s_matrix, 100, jnp.eye(s_matrix.shape[0]), lr=1.)\n",
        "\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(opt)\n",
        "print(losses)\n",
        "print(jnp.min(jnp.linalg.eigh(opt)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omsdtGuY2REV"
      },
      "source": [
        "# Algorithm implementation: fixed-point iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SVr9OPcvVY1t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opt array: [[1.52777994 1.23632647 1.11701383 ... 0.01181435 0.00787596 0.00393792]\n",
            " [1.23632647 1.4393963  1.16966251 ... 0.01181819 0.00787853 0.0039392 ]\n",
            " [1.11701383 1.16966251 1.40149837 ... 0.0118253  0.00788326 0.00394157]\n",
            " ...\n",
            " [0.01181435 0.01181819 0.0118253  ... 1.04018386 0.44807401 0.20139658]\n",
            " [0.00787596 0.00787853 0.00788326 ... 0.44807401 1.00947009 0.3375872 ]\n",
            " [0.00393792 0.0039392  0.00394157 ... 0.20139658 0.3375872  0.94777817]]\n",
            "Time to compute: [0.0, 0.06099987030029297, 0.07012391090393066]\n",
            "losses: [Array(685.56154706, dtype=float64), Array(684.63674222, dtype=float64), Array(684.19630193, dtype=float64)]\n"
          ]
        }
      ],
      "source": [
        "def compute_phi_fixed_point(\n",
        "    matrix: jnp.ndarray,\n",
        "    initial_v: jnp.array,\n",
        "    rtol: float = 1e-5,\n",
        "    max_iterations: Optional[int] = None,\n",
        ") -> Tuple[jnp.ndarray, int, float]:\n",
        "\n",
        "  target = hermitian_adjoint(matrix) @ matrix \n",
        "  v = initial_v\n",
        "\n",
        "  n_iters = 0\n",
        "\n",
        "  def continue_loop(iteration: int) -> bool:\n",
        "    if max_iterations is None:\n",
        "      return True\n",
        "    return iteration < max_iterations\n",
        "\n",
        "  @jit\n",
        "  def _compute_loss(v, matrix_sqrt):\n",
        "    # This computation in the middle may slow down our fixed point method and could\n",
        "    # be bypassed. We only have it here to track the loss as we iterate.\n",
        "    normalized_x = compute_normalized_x_from_vector(matrix, v, matrix_sqrt)\n",
        "    loss = compute_loss_in_x(matrix, normalized_x)\n",
        "    return loss\n",
        "\n",
        "  def _update_loss(v, matrix_sqrt):\n",
        "    loss = _compute_loss(v, matrix_sqrt)\n",
        "    # We rely on Python late binding to capture start here.\n",
        "    time_array.append(time.time() - start)\n",
        "    loss_array.append(loss)\n",
        "\n",
        "  time_array = []\n",
        "  loss_array = []\n",
        "  # We keep around the previously computed matrix square root\n",
        "  # to save time in evaluating loss.\n",
        "  matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(v) ** 0.5 @ target @ jnp.diag(v) ** 0.5)\n",
        "\n",
        "  start = time.time()\n",
        "  while continue_loop(n_iters):\n",
        "    n_iters += 1\n",
        "    # Compute loss first, for first iteration, to normalize the loss trajectories\n",
        "    # between these loss arrays and the descent-based ones.\n",
        "    _update_loss(v, matrix_sqrt)\n",
        "    diag = jnp.diag(v)\n",
        "    diag_sqrt = diag ** 0.5\n",
        "    new_v = jnp.diag(matrix_sqrt)\n",
        "    # Set up matrix_sqrt for the next iteration. We use this wonky update order to be\n",
        "    # able to cache this square root computation for loss evaluation.\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(new_v) ** 0.5 @ target @ jnp.diag(new_v) ** 0.5)\n",
        "    norm_diff = jnp.linalg.norm(new_v - v)\n",
        "    rel_norm_diff = norm_diff / jnp.linalg.norm(v)\n",
        "    if rel_norm_diff < rtol:\n",
        "      _update_loss(new_v, matrix_sqrt)\n",
        "      return new_v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "    v = new_v\n",
        "\n",
        "  _update_loss(v, matrix_sqrt)\n",
        "  return v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "\n",
        "def optimize_factorization_fixed_point(s_matrix: jnp.ndarray, max_iterations: int, rtol: float, initial_v: Optional[jnp.array]=None):\n",
        "  if initial_v is None:\n",
        "    initial_v = jnp.ones_like(jnp.diag(s_matrix))\n",
        "  (lagrange_multiplier, n_iters,\n",
        "   final_relnorm, timing, losses) = compute_phi_fixed_point(\n",
        "       s_matrix, rtol=rtol, max_iterations=max_iterations, initial_v=initial_v)\n",
        "   \n",
        "  inv_diag_sqrt = jnp.diag(lagrange_multiplier**-(0.5))\n",
        "  diag_sqrt = jnp.diag(lagrange_multiplier**0.5)\n",
        "\n",
        "  target = hermitian_adjoint(s_matrix) @ s_matrix\n",
        "  x = inv_diag_sqrt @ diagonalize_and_take_jax_matrix_sqrt(\n",
        "      diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt) @ inv_diag_sqrt\n",
        "\n",
        "  # Suppress any costs to the first iteration, often due to tracing, etc\n",
        "  initial_time = timing[0]\n",
        "  adj_time_array = [x - initial_time for x in timing]\n",
        "\n",
        "  return x, losses, adj_time_array\n",
        "\n",
        "\n",
        "opt, loss, time_to_compute = optimize_factorization_fixed_point(s_matrix, 2, rtol=1e-1)\n",
        "print(f'Opt array: {opt}')\n",
        "print(f'Time to compute: {time_to_compute}')\n",
        "print(f'losses: {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1nq1rasK2w"
      },
      "source": [
        "# Newton-step-style algorithm from [existing literature](https://arxiv.org/pdf/1602.04302v1.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IU7CO7TAgEJR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final loss: [Array(8256., dtype=float64), Array(5542.63719423, dtype=float64), Array(2847.64931587, dtype=float64), Array(1534.60983352, dtype=float64), Array(1386.83297443, dtype=float64), Array(943.14572093, dtype=float64), Array(868.39020584, dtype=float64), Array(857.89685045, dtype=float64), Array(847.14937673, dtype=float64), Array(843.53326629, dtype=float64), Array(835.35122893, dtype=float64), Array(828.55243871, dtype=float64), Array(777.4971529, dtype=float64), Array(773.9535248, dtype=float64), Array(761.10506948, dtype=float64), Array(752.1550407, dtype=float64), Array(749.76137602, dtype=float64), Array(747.1469614, dtype=float64), Array(745.14661879, dtype=float64), Array(738.15446089, dtype=float64), Array(734.79461947, dtype=float64), Array(730.44398639, dtype=float64), Array(728.67582355, dtype=float64), Array(727.45791161, dtype=float64), Array(722.30818594, dtype=float64), Array(718.98782356, dtype=float64), Array(717.7370258, dtype=float64), Array(716.91753463, dtype=float64), Array(716.19019102, dtype=float64), Array(715.52257545, dtype=float64), Array(714.87651001, dtype=float64), Array(710.85843437, dtype=float64), Array(710.15637335, dtype=float64), Array(708.99834523, dtype=float64), Array(708.22806403, dtype=float64), Array(707.66526858, dtype=float64), Array(707.21270433, dtype=float64), Array(706.77947556, dtype=float64), Array(703.96779652, dtype=float64), Array(703.52036988, dtype=float64), Array(703.25610141, dtype=float64), Array(702.6754904, dtype=float64), Array(702.30783803, dtype=float64), Array(701.86915457, dtype=float64), Array(701.54647683, dtype=float64), Array(701.21980433, dtype=float64), Array(700.92183207, dtype=float64), Array(700.62444148, dtype=float64), Array(700.34075085, dtype=float64), Array(700.06028452, dtype=float64), Array(699.78889729, dtype=float64), Array(699.52170912, dtype=float64), Array(699.26162418, dtype=float64), Array(699.0060443, dtype=float64), Array(698.14602563, dtype=float64), Array(697.53548455, dtype=float64), Array(697.22072872, dtype=float64), Array(697.01692738, dtype=float64), Array(696.56023439, dtype=float64), Array(696.47332258, dtype=float64), Array(696.24040282, dtype=float64), Array(696.00804411, dtype=float64), Array(695.82010696, dtype=float64), Array(695.61626271, dtype=float64), Array(695.43673899, dtype=float64), Array(695.24639371, dtype=float64), Array(695.0736545, dtype=float64), Array(694.89299548, dtype=float64), Array(694.72665954, dtype=float64), Array(694.5539983, dtype=float64), Array(694.39385386, dtype=float64), Array(694.22821749, dtype=float64), Array(694.07409754, dtype=float64), Array(693.91482359, dtype=float64), Array(693.76658873, dtype=float64), Array(693.61316992, dtype=float64), Array(693.47071007, dtype=float64), Array(693.32272244, dtype=float64), Array(693.18596303, dtype=float64), Array(693.04302661, dtype=float64), Array(692.91193648, dtype=float64), Array(692.77368977, dtype=float64), Array(692.64828995, dtype=float64), Array(692.51436983, dtype=float64), Array(692.39474269, dtype=float64), Array(692.26476573, dtype=float64), Array(692.15106324, dtype=float64), Array(692.02460673, dtype=float64), Array(691.91705554, dtype=float64), Array(691.79363807, dtype=float64), Array(691.69253796, dtype=float64), Array(691.57160153, dtype=float64), Array(691.47731255, dtype=float64), Array(691.35821097, dtype=float64), Array(691.3226721, dtype=float64), Array(691.22419569, dtype=float64), Array(691.11502151, dtype=float64), Array(691.02496456, dtype=float64), Array(690.91761241, dtype=float64), Array(690.88646855, dtype=float64)]\n",
            "Time in loop: [0.0, 0.021927833557128906, 0.0400998592376709, 0.05814862251281738, 0.07575154304504395, 0.09685683250427246, 0.11753463745117188, 0.19696640968322754, 0.21822500228881836, 0.23870325088500977, 0.2593350410461426, 0.2788252830505371, 0.2983670234680176, 0.3176860809326172, 0.33922839164733887, 0.3602714538574219, 0.3795626163482666, 0.3989601135253906, 0.4195742607116699, 0.44277119636535645, 0.461284875869751, 0.48027777671813965, 0.5000848770141602, 0.5200269222259521, 0.5389719009399414, 0.5603220462799072, 0.6336894035339355, 0.6629395484924316, 0.6864116191864014, 0.7102179527282715, 0.7308075428009033, 0.7546508312225342, 0.7752163410186768, 0.7955877780914307, 0.816187858581543, 0.8397972583770752, 0.8632490634918213, 0.8835468292236328, 0.9069936275482178, 0.9631950855255127, 0.9819819927215576, 1.000678300857544, 1.0200433731079102, 1.0383050441741943, 1.060014009475708, 1.0821847915649414, 1.101527214050293, 1.1236538887023926, 1.142482042312622, 1.1641604900360107, 1.1872427463531494, 1.2087647914886475, 1.2307336330413818, 1.2767610549926758, 1.3583221435546875, 1.3820323944091797, 1.4056038856506348, 1.425922155380249, 1.4495000839233398, 1.4702165126800537, 1.4909698963165283, 1.5112996101379395, 1.5317552089691162, 1.5520892143249512, 1.572446584701538, 1.5965707302093506, 1.6171214580535889, 1.6404094696044922, 1.6608643531799316, 1.6847875118255615, 1.7051215171813965, 1.7284326553344727, 1.748605489730835, 1.7719752788543701, 1.7927510738372803, 1.817884922027588, 1.8399646282196045, 1.8699023723602295, 1.8915579319000244, 1.9149339199066162, 1.9355247020721436, 1.9594576358795166, 1.9802112579345703, 2.004185438156128, 2.024660110473633, 2.0449044704437256, 2.0657923221588135, 2.086109161376953, 2.1064953804016113, 2.1256425380706787, 2.145930051803589, 2.165475368499756, 2.1854355335235596, 2.2076942920684814, 2.227342367172241, 2.246946334838867, 2.2659735679626465, 2.2849714756011963, 2.3081488609313965, 2.329829692840576]\n",
            "Min eval: 0.1577593517722355\n"
          ]
        }
      ],
      "source": [
        "# Implementing Alg 1 from https://arxiv.org/pdf/1602.04302v1.pdf.\n",
        "\n",
        "def compute_newton_direction(Z, grad, max_iter: int = 5):\n",
        "  \"\"\"Implements algorithm 2 from the referenced paper.\"\"\"\n",
        "  # Initialize according to line 4.\n",
        "  D = jnp.zeros_like(grad)\n",
        "  R = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "  # Set diag of D and R to zero; line 5\n",
        "  diag_elements = jnp.diag_indices_from(D)\n",
        "  D = D.at[diag_elements].set(0)\n",
        "  R = R.at[diag_elements].set(0)\n",
        "  # Initialize P and r_old as in line 6.\n",
        "  P = R\n",
        "  # Interestingly, this is the inner product used in the paper.\n",
        "  r_old = jnp.sum(R * R)\n",
        "  for i in range(max_iter):\n",
        "    # Set B and alpha as in line 8\n",
        "    B = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "    alpha = r_old / jnp.sum(P * B)\n",
        "    # Update D and R as in line 9\n",
        "    D = D + alpha * P\n",
        "    R = R - alpha * B\n",
        "    # Set diags of D anr R to 0, as in line 10\n",
        "    D = D.at[diag_elements].set(0)\n",
        "    R = R.at[diag_elements].set(0)\n",
        "    # Set r_new and update P, as in line 11\n",
        "    r_new = jnp.sum(R * R)\n",
        "    P = R + r_new / r_old * P\n",
        "    # Update r_old; line 12\n",
        "    r_old = r_new\n",
        "    if jnp.max(jnp.abs(R)) == 0:\n",
        "      # Everything nans if this is violated. I assume this loop should terminate in this case.\n",
        "      break\n",
        "  return D\n",
        "\n",
        "def optimize_factorization_newton_step(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, init_lr=1.):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Setting to 1 reproduces the paper of interest; see section 4.2.\n",
        "  # We parameterize for the purposes of tuning.\n",
        "  lr = init_lr\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr):\n",
        "    \"\"\"Computes step size as in Sec 4.2 of referenced paper.\"\"\"\n",
        "    candidate = x_iter + newton_dir * init_lr\n",
        "    # This is essentially the method for checking positive-definiteness proposed\n",
        "    # by the paper.\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "    # Equation (16)\n",
        "    target_decrease = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad * newton_dir)\n",
        "    if compiled_loss(candidate) <= target_decrease:\n",
        "      return candidate\n",
        "    return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compiled_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    inv_x = jnp.linalg.inv(x_iter)\n",
        "    newton_direction = compute_newton_direction(Z=inv_x, grad=grad)\n",
        "    loss_array.append(loss)\n",
        "    x_iter = find_next_iterate_armijo(x_iter, grad, newton_direction, lr)\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2 \n",
        "    time_array.append(time.time() - start)\n",
        "\n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "opt, losses, time_in_loop = optimize_factorization_newton_step(s_matrix, 100, jnp.eye(s_matrix.shape[0]))\n",
        "print(f'Final loss: {losses}')\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(f'Min eval: {jnp.min(jnp.linalg.eigh(opt)[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dseTAZa-Dm9t"
      },
      "source": [
        "# Data and plot generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ekxkv8DDMKqq"
      },
      "outputs": [],
      "source": [
        "_MAX_ITERS = 1000\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def matrix_factorization_speed_data(matrix, dim: int, max_inv_rtol: int = 20, max_iter: int = 1000):\n",
        "\n",
        "  # We fix a seed for reproducability, though the results are generally uniform\n",
        "  # in this seed.\n",
        "  key = jax.random.PRNGKey(256)\n",
        "  initial_v = jax.random.uniform(key=key, shape=jnp.diag(matrix).shape)\n",
        "\n",
        "  fp_data = {'x': [], 'y': []}\n",
        "  gd_data = {'x': [], 'y': []}\n",
        "  ns_data = {'x': [], 'y': []}\n",
        "\n",
        "  _, losses_fp, time_to_compute_fp = optimize_factorization_fixed_point(matrix, _MAX_ITERS, rtol=10**-max_inv_rtol, initial_v=initial_v)\n",
        "  fp_data['x'] = time_to_compute_fp\n",
        "  fp_data['y'] = [float(x.to_py()) for x in losses_fp]\n",
        "\n",
        "  initial_x = compute_normalized_x_from_vector(matrix, initial_v)\n",
        "\n",
        "  n_iters = max_iter\n",
        "  # True gradient descent on the convex problem\n",
        "  _, losses, time_in_loop = optimize_factorization_grad_descent(matrix, n_iters, initial_x, lr=1., use_armijo_rule=True)\n",
        "  gd_data['x'] = time_in_loop\n",
        "  gd_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  # The Newton-direction-based method of https://arxiv.org/pdf/1602.04302v1.pdf\n",
        "  _, losses, time_in_loop = optimize_factorization_newton_step(matrix, n_iters, initial_x, init_lr=1.)\n",
        "  ns_data['x'] = time_in_loop\n",
        "  ns_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  df = pd.DataFrame({'Elapsed Time (s)': fp_data['x'] + gd_data['x'] + ns_data['x'], \n",
        "                   'Loss': fp_data['y'] + gd_data['y'] + ns_data['y'],\n",
        "                   'Method': ['Fixed point'] * len(fp_data['x']) + ['Gradient descent'] * len(gd_data['x']) + ['Newton-based step'] * len(ns_data['x'])})\n",
        "  return df\n",
        "\n",
        "\n",
        "def prefix_sum_factorization_speed_data(dim: int, max_inv_rtol: int = 20, max_iter: int = 1000) -> pd.DataFrame:\n",
        "  s_matrix = jnp.tril(jnp.ones(shape=(dim, dim)))\n",
        "  return matrix_factorization_speed_data(s_matrix, dim, max_inv_rtol, max_iter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzVm0QGVSa6g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-20 11:17:18.879095: E external/xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:\n",
            "\n",
            "  %dot.93 = f64[2048,2048]{1,0} dot(f64[2048,2048]{0,1} %constant.214, f64[2048,2048]{1,0} %constant.6), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(_compute_loss)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_591122/3330464615.py\" source_line=28}\n",
            "\n",
            "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
            "\n",
            "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
            "2025-05-20 11:17:55.547377: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 37.668553509s\n",
            "Constant folding an instruction is taking > 1s:\n",
            "\n",
            "  %dot.93 = f64[2048,2048]{1,0} dot(f64[2048,2048]{0,1} %constant.214, f64[2048,2048]{1,0} %constant.6), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(_compute_loss)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_591122/3330464615.py\" source_line=28}\n",
            "\n",
            "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
            "\n",
            "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
          ]
        }
      ],
      "source": [
        "def generate_and_plot_data(dim: int, max_inv_rtol: int, max_iter: int):\n",
        "\n",
        "  df = prefix_sum_factorization_speed_data(dim=dim, max_inv_rtol=max_inv_rtol, max_iter=max_iter)\n",
        "  palette = sns.color_palette('muted')\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  sns.set_style('whitegrid')\n",
        "  sns.set_context('paper')\n",
        "  line = sns.lineplot(data=df,\n",
        "              x='Elapsed Time (s)',\n",
        "              y='Loss',\n",
        "              hue='Method',\n",
        "              palette=[palette[0], palette[1], palette[2]],\n",
        "              )\n",
        "\n",
        "  # Compute the max time that all methods generated data for.\n",
        "  max_elapsed_times = []\n",
        "  for method in df['Method'].unique():\n",
        "    max_elapsed_times.append(np.max(df[df['Method'] == method]['Elapsed Time (s)']))\n",
        "  max_all_elapsed_time = max(max_elapsed_times)\n",
        "\n",
        "  max_loss = np.max(df['Loss'])\n",
        "  min_loss = np.min(df['Loss'])\n",
        "\n",
        "  # Heuristic method to set the ranges for visibility\n",
        "  plt.ylim(min_loss - (max_loss - min_loss) * 0.05, max_loss)\n",
        "  plt.xlim(0, max_all_elapsed_time)\n",
        "  return df \n",
        "\n",
        "df = generate_and_plot_data(2048, 10, 10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Prefix-sum matrix factorization optimization.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1eY_VCuTGPrvwhGEH8f3QKNzAtAw-BkPC",
          "timestamp": 1653448381403
        },
        {
          "file_id": "1Wfhb9XY3uGWfA8jm2gYdx28JhODLwcho",
          "timestamp": 1652721590159
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python (dp-matfac-env)",
      "language": "python",
      "name": "dp-matfac-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
