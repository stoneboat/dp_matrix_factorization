{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYDkIhdN0y9"
      },
      "source": [
        "In this notebook we implement three different methods for computing optimal factorizations for [the matrix mechanism](https://people.cs.umass.edu/~mcgregor/papers/15-vldbj.pdf) under approximate differential privacy:\n",
        "\n",
        "1. Gradient descent on an associated convex problem\n",
        "1. The fixed-point iteration method described in the paper associated to this code.\n",
        "1. A Newton-direction-based algorithm designed in [previous literature](https://arxiv.org/pdf/1602.04302v1.pdf).\n",
        "\n",
        "We experimentally compare their numerical efficiency on the problem of factorizing the prefix-sum matrix: the lower-triangular matrix of all 1s, which takes a vector to its vector of partial sums. Other matrices can be used, and will generally produce similar results.\n",
        "\n",
        "One note on initialization below: it is difficult to initialize the two descent-based schemes and the fixed-point based algorithm identically. In order to ensure similar initialization, it is easiest to select a *vector* (corresponding to the parameterization of the fixed-point algorithm), and attempt to generate a matrix from this vector. Generating this matrix, however, essentially takes advantage of the representations which yield the fixed-point problem--and this generated matrix usually has significantly lower loss than a general positive definite matrix with constant 1s on the diagonal. This observation is not necessarily surprising; the dimensionalities involved in a vector parameterization are much lower. It is slightly unfair to the fixed-point method to 'allow' the gradient-based methods to use this initialization; but since the fixed-point method is the one we propose, we reserve the right to make it look slightly worse than it otherwise might."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J1Z1q8omN1sc"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "import time\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import value_and_grad, jit\n",
        "from jax import config\n",
        "import numpy as np\n",
        "\n",
        "# With large matrices, the extra precision afforded by performing all\n",
        "# computations in float64 is critical.\n",
        "config.update('jax_enable_x64', True)\n",
        "\n",
        "\n",
        "@jit\n",
        "def diagonalize_and_take_jax_matrix_sqrt(matrix: jnp.ndarray, min_eval: float = 0.0) -> jnp.ndarray:\n",
        "  \"\"\"Matrix square root for positive-semi-definite, Hermitian matrices.\"\"\"\n",
        "  evals, evecs = jnp.linalg.eigh(matrix)\n",
        "  eval_sqrt = jnp.maximum(evals, min_eval)**0.5\n",
        "  sqrt = evecs @ jnp.diag(eval_sqrt) @ evecs.T\n",
        "  return sqrt\n",
        "\n",
        "def hermitian_adjoint(matrix: jnp.ndarray) -> jnp.ndarray:\n",
        "  return jnp.conjugate(matrix).T\n",
        "\n",
        "def compute_loss_in_x(target: jnp.ndarray, x: jnp.ndarray):\n",
        "  m = hermitian_adjoint(target) @ target @ jnp.linalg.inv(x)\n",
        "  raw_trace = jnp.trace(m)\n",
        "  max_diag = jnp.max(jnp.diag(x))\n",
        "  return raw_trace * max_diag\n",
        "\n",
        "def compute_normalized_x_from_vector(matrix_to_factorize, v, precomputed_sqrt: Optional[jnp.ndarray] = None):\n",
        "  \"\"\"Computes a normalized (to all-1s diagonal) version of the vector -> matrix\n",
        "  mapping which defines the relationship between fixed points and optima.\n",
        "\n",
        "  At a fixed point of phi (equivalently an optimum of the symmetrized\n",
        "  factorization problem), the normalization below will no-op. But to normalize\n",
        "  between iterations of the fixed-point method and iterations of the descent-\n",
        "  based methods, it is useful to force the results of this transformation\n",
        "  to always have constant-1 diagonals.\n",
        "  \"\"\"\n",
        "  inv_diag_sqrt = jnp.diag(v ** -(0.5))\n",
        "  diag_sqrt = jnp.diag(v ** 0.5)\n",
        "  if precomputed_sqrt is None:\n",
        "    target = hermitian_adjoint(matrix_to_factorize) @ matrix_to_factorize\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(\n",
        "        diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt)\n",
        "  else:\n",
        "    # We simply assume that our caller did this computation correctly.\n",
        "    matrix_sqrt = precomputed_sqrt\n",
        "  x = inv_diag_sqrt @ matrix_sqrt @ inv_diag_sqrt\n",
        "  # Force all-1s on diagonal. This normalization is a requirement for the\n",
        "  # initial iterates of the descent-based methods, and we know it's true at the\n",
        "  # optimum.\n",
        "  x_sqrt_diag = jnp.diag(x)\n",
        "  normalized_x = jnp.diag((x_sqrt_diag)**(-0.5)) @ x @ jnp.diag((x_sqrt_diag)**(-0.5))\n",
        "  return normalized_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06vyKu8DqKN"
      },
      "source": [
        "# Algorithm implementation: gradient descent on the convex problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0GonuoXsN6gP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time in loop: [0.0, 0.06113100051879883, 0.12260103225708008, 0.18252873420715332, 0.2446129322052002, 0.3147714138031006, 0.3872663974761963, 0.45621347427368164, 0.5198268890380859, 0.5857405662536621, 0.6506350040435791, 0.712977409362793, 0.7795140743255615, 0.841895580291748, 0.8958606719970703, 0.9495019912719727, 0.9966433048248291, 1.0400145053863525, 1.0840966701507568, 1.1292226314544678, 1.1737582683563232, 1.2418639659881592, 1.2859151363372803, 1.337432622909546, 1.3989262580871582, 1.4425008296966553, 1.4868340492248535, 1.5303616523742676, 1.5758352279663086, 1.6433320045471191, 1.7121737003326416, 1.756495475769043, 1.8239116668701172, 1.867523193359375, 1.9117240905761719, 1.9550516605377197, 1.9989521503448486, 2.041843891143799, 2.0861129760742188, 2.1302642822265625, 2.1749355792999268, 2.218355894088745, 2.2623445987701416, 2.3142929077148438, 2.3590428829193115, 2.4027721881866455, 2.4471044540405273, 2.490889549255371, 2.5352447032928467, 2.6018502712249756, 2.6499176025390625, 2.718315601348877, 2.7625226974487305, 2.8328495025634766, 2.8759140968322754, 2.9430787563323975, 2.9871881008148193, 3.053253173828125, 3.096710681915283, 3.163844347000122, 3.2087416648864746, 3.2785353660583496, 3.3263051509857178, 3.436190128326416, 3.499403476715088, 3.6026628017425537, 3.6602110862731934, 3.7638494968414307, 3.8273260593414307, 3.916253089904785, 3.976608991622925, 4.065737962722778, 4.15913987159729, 4.243263483047485, 4.300612926483154, 4.392292737960815, 4.454923391342163, 4.54202127456665, 4.597989797592163, 4.685669660568237, 4.7487242221832275, 4.844966650009155, 4.900150299072266, 4.955041170120239, 5.009852647781372, 5.0657572746276855, 5.119740962982178, 5.171513557434082, 5.227556467056274, 5.278899669647217, 5.327463865280151, 5.391766309738159, 5.450066089630127, 5.511784791946411, 5.5641028881073, 5.607967138290405, 5.651166915893555, 5.697232961654663, 5.741461992263794, 5.784806728363037]\n",
            "[[1.         0.82342968 0.79046125 ... 0.0068329  0.00455158 0.00227372]\n",
            " [0.82342968 1.         0.81142944 ... 0.00679745 0.00452663 0.00226022]\n",
            " [0.79046125 0.81142944 1.         ... 0.00677246 0.00450926 0.00225104]\n",
            " ...\n",
            " [0.0068329  0.00679745 0.00677246 ... 1.         0.42517039 0.1849818 ]\n",
            " [0.00455158 0.00452663 0.00450926 ... 0.42517039 1.         0.325569  ]\n",
            " [0.00227372 0.00226022 0.00225104 ... 0.1849818  0.325569   1.        ]]\n",
            "[DeviceArray(8256., dtype=float64), DeviceArray(2104.27898517, dtype=float64), DeviceArray(1463.20285549, dtype=float64), DeviceArray(1342.11620743, dtype=float64), DeviceArray(898.68442588, dtype=float64), DeviceArray(854.2611167, dtype=float64), DeviceArray(845.62003445, dtype=float64), DeviceArray(836.9059374, dtype=float64), DeviceArray(829.75063811, dtype=float64), DeviceArray(823.42569101, dtype=float64), DeviceArray(817.63257377, dtype=float64), DeviceArray(772.81886588, dtype=float64), DeviceArray(769.88399766, dtype=float64), DeviceArray(766.58666863, dtype=float64), DeviceArray(751.34849153, dtype=float64), DeviceArray(745.69758548, dtype=float64), DeviceArray(743.87093445, dtype=float64), DeviceArray(741.54531554, dtype=float64), DeviceArray(739.79355655, dtype=float64), DeviceArray(729.77809885, dtype=float64), DeviceArray(732.98700254, dtype=float64), DeviceArray(732.75470826, dtype=float64), DeviceArray(729.96054992, dtype=float64), DeviceArray(726.50141681, dtype=float64), DeviceArray(724.52076941, dtype=float64), DeviceArray(723.14115161, dtype=float64), DeviceArray(722.01326771, dtype=float64), DeviceArray(721.04935302, dtype=float64), DeviceArray(717.76510309, dtype=float64), DeviceArray(713.92043327, dtype=float64), DeviceArray(713.03204186, dtype=float64), DeviceArray(712.39761558, dtype=float64), DeviceArray(710.95644731, dtype=float64), DeviceArray(707.74563308, dtype=float64), DeviceArray(706.93471389, dtype=float64), DeviceArray(706.32374253, dtype=float64), DeviceArray(704.79689553, dtype=float64), DeviceArray(710.26297197, dtype=float64), DeviceArray(705.50123981, dtype=float64), DeviceArray(702.66799677, dtype=float64), DeviceArray(701.7733758, dtype=float64), DeviceArray(701.28171963, dtype=float64), DeviceArray(700.9265133, dtype=float64), DeviceArray(698.9218984, dtype=float64), DeviceArray(698.36420357, dtype=float64), DeviceArray(697.80245687, dtype=float64), DeviceArray(697.54844471, dtype=float64), DeviceArray(697.30150078, dtype=float64), DeviceArray(697.08893183, dtype=float64), DeviceArray(696.86422506, dtype=float64), DeviceArray(696.65945896, dtype=float64), DeviceArray(696.44814742, dtype=float64), DeviceArray(696.25088281, dtype=float64), DeviceArray(696.05008644, dtype=float64), DeviceArray(695.86011613, dtype=float64), DeviceArray(695.66826187, dtype=float64), DeviceArray(695.48532825, dtype=float64), DeviceArray(695.30143273, dtype=float64), DeviceArray(695.12526672, dtype=float64), DeviceArray(694.94864397, dtype=float64), DeviceArray(694.77898377, dtype=float64), DeviceArray(694.60911191, dtype=float64), DeviceArray(694.44571142, dtype=float64), DeviceArray(694.28216622, dtype=float64), DeviceArray(694.12479964, dtype=float64), DeviceArray(693.96721877, dtype=float64), DeviceArray(693.81568406, dtype=float64), DeviceArray(693.66374636, dtype=float64), DeviceArray(693.51786896, dtype=float64), DeviceArray(693.37128107, dtype=float64), DeviceArray(693.23091864, dtype=float64), DeviceArray(693.08940518, dtype=float64), DeviceArray(692.95445389, dtype=float64), DeviceArray(692.81774879, dtype=float64), DeviceArray(692.68815172, dtype=float64), DeviceArray(692.55598904, dtype=float64), DeviceArray(692.4317471, dtype=float64), DeviceArray(692.30384972, dtype=float64), DeviceArray(692.18503518, dtype=float64), DeviceArray(692.06109986, dtype=float64), DeviceArray(691.94787182, dtype=float64), DeviceArray(691.82754905, dtype=float64), DeviceArray(691.72016907, dtype=float64), DeviceArray(691.60303679, dtype=float64), DeviceArray(691.50188011, dtype=float64), DeviceArray(691.38741202, dtype=float64), DeviceArray(691.29296712, dtype=float64), DeviceArray(691.18050008, dtype=float64), DeviceArray(691.09334568, dtype=float64), DeviceArray(690.98205682, dtype=float64), DeviceArray(690.90280545, dtype=float64), DeviceArray(690.79171673, dtype=float64), DeviceArray(690.72092023, dtype=float64), DeviceArray(690.60895157, dtype=float64), DeviceArray(690.54698115, dtype=float64), DeviceArray(690.43306445, dtype=float64), DeviceArray(690.38000101, dtype=float64), DeviceArray(690.26324101, dtype=float64), DeviceArray(690.21882562, dtype=float64), DeviceArray(690.09865831, dtype=float64)]\n",
            "0.1579340745210083\n"
          ]
        }
      ],
      "source": [
        "def optimize_factorization_grad_descent(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, lr: float = 1., use_armijo_rule: bool = True):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate(x_iter, grad, init_lr):\n",
        "    candidate = x_iter - grad * init_lr\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "    else:\n",
        "      sufficient_decrease_condition = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad ** 2)\n",
        "      if compiled_loss(candidate) <= sufficient_decrease_condition:\n",
        "        return candidate\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compute_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    diag_elements = jnp.diag_indices_from(grad)\n",
        "    grad1 = grad.at[diag_elements].set(0)\n",
        "    loss_array.append(loss)\n",
        "    if use_armijo_rule:\n",
        "      x_iter = find_next_iterate(x_iter, grad1, lr)\n",
        "    else:\n",
        "      x_iter = x_iter - lr * grad1\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2\n",
        "    \n",
        "    time_array.append(time.time() - start)\n",
        "  \n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "s_matrix = jnp.tril(jnp.ones(shape=(128, 128)))\n",
        "opt, losses, time_in_loop = optimize_factorization_grad_descent(s_matrix, 100, jnp.eye(s_matrix.shape[0]), lr=1.)\n",
        "\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(opt)\n",
        "print(losses)\n",
        "print(jnp.min(jnp.linalg.eigh(opt)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omsdtGuY2REV"
      },
      "source": [
        "# Algorithm implementation: fixed-point iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SVr9OPcvVY1t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opt array: [[1.52777994 1.23632647 1.11701383 ... 0.01181435 0.00787596 0.00393792]\n",
            " [1.23632647 1.4393963  1.16966251 ... 0.01181819 0.00787853 0.0039392 ]\n",
            " [1.11701383 1.16966251 1.40149837 ... 0.0118253  0.00788326 0.00394157]\n",
            " ...\n",
            " [0.01181435 0.01181819 0.0118253  ... 1.04018386 0.44807401 0.20139658]\n",
            " [0.00787596 0.00787853 0.00788326 ... 0.44807401 1.00947009 0.3375872 ]\n",
            " [0.00393792 0.0039392  0.00394157 ... 0.20139658 0.3375872  0.94777817]]\n",
            "Time to compute: [0.0, 0.11228299140930176, 0.1315293312072754]\n",
            "losses: [DeviceArray(685.56154706, dtype=float64), DeviceArray(684.63674222, dtype=float64), DeviceArray(684.19630193, dtype=float64)]\n"
          ]
        }
      ],
      "source": [
        "def compute_phi_fixed_point(\n",
        "    matrix: jnp.ndarray,\n",
        "    initial_v: jnp.array,\n",
        "    rtol: float = 1e-5,\n",
        "    max_iterations: Optional[int] = None,\n",
        ") -> Tuple[jnp.ndarray, int, float]:\n",
        "\n",
        "  target = hermitian_adjoint(matrix) @ matrix \n",
        "  v = initial_v\n",
        "\n",
        "  n_iters = 0\n",
        "\n",
        "  def continue_loop(iteration: int) -> bool:\n",
        "    if max_iterations is None:\n",
        "      return True\n",
        "    return iteration < max_iterations\n",
        "\n",
        "  @jit\n",
        "  def _compute_loss(v, matrix_sqrt):\n",
        "    # This computation in the middle may slow down our fixed point method and could\n",
        "    # be bypassed. We only have it here to track the loss as we iterate.\n",
        "    normalized_x = compute_normalized_x_from_vector(matrix, v, matrix_sqrt)\n",
        "    loss = compute_loss_in_x(matrix, normalized_x)\n",
        "    return loss\n",
        "\n",
        "  def _update_loss(v, matrix_sqrt):\n",
        "    loss = _compute_loss(v, matrix_sqrt)\n",
        "    # We rely on Python late binding to capture start here.\n",
        "    time_array.append(time.time() - start)\n",
        "    loss_array.append(loss)\n",
        "\n",
        "  time_array = []\n",
        "  loss_array = []\n",
        "  # We keep around the previously computed matrix square root\n",
        "  # to save time in evaluating loss.\n",
        "  matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(v) ** 0.5 @ target @ jnp.diag(v) ** 0.5)\n",
        "\n",
        "  start = time.time()\n",
        "  while continue_loop(n_iters):\n",
        "    n_iters += 1\n",
        "    # Compute loss first, for first iteration, to normalize the loss trajectories\n",
        "    # between these loss arrays and the descent-based ones.\n",
        "    _update_loss(v, matrix_sqrt)\n",
        "    diag = jnp.diag(v)\n",
        "    diag_sqrt = diag ** 0.5\n",
        "    new_v = jnp.diag(matrix_sqrt)\n",
        "    # Set up matrix_sqrt for the next iteration. We use this wonky update order to be\n",
        "    # able to cache this square root computation for loss evaluation.\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(new_v) ** 0.5 @ target @ jnp.diag(new_v) ** 0.5)\n",
        "    norm_diff = jnp.linalg.norm(new_v - v)\n",
        "    rel_norm_diff = norm_diff / jnp.linalg.norm(v)\n",
        "    if rel_norm_diff < rtol:\n",
        "      _update_loss(new_v, matrix_sqrt)\n",
        "      return new_v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "    v = new_v\n",
        "\n",
        "  _update_loss(v, matrix_sqrt)\n",
        "  return v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "\n",
        "def optimize_factorization_fixed_point(s_matrix: jnp.ndarray, max_iterations: int, rtol: float, initial_v: Optional[jnp.array]=None):\n",
        "  if initial_v is None:\n",
        "    initial_v = jnp.ones_like(jnp.diag(s_matrix))\n",
        "  (lagrange_multiplier, n_iters,\n",
        "   final_relnorm, timing, losses) = compute_phi_fixed_point(\n",
        "       s_matrix, rtol=rtol, max_iterations=max_iterations, initial_v=initial_v)\n",
        "   \n",
        "  inv_diag_sqrt = jnp.diag(lagrange_multiplier**-(0.5))\n",
        "  diag_sqrt = jnp.diag(lagrange_multiplier**0.5)\n",
        "\n",
        "  target = hermitian_adjoint(s_matrix) @ s_matrix\n",
        "  x = inv_diag_sqrt @ diagonalize_and_take_jax_matrix_sqrt(\n",
        "      diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt) @ inv_diag_sqrt\n",
        "\n",
        "  # Suppress any costs to the first iteration, often due to tracing, etc\n",
        "  initial_time = timing[0]\n",
        "  adj_time_array = [x - initial_time for x in timing]\n",
        "\n",
        "  return x, losses, adj_time_array\n",
        "\n",
        "\n",
        "opt, loss, time_to_compute = optimize_factorization_fixed_point(s_matrix, 2, rtol=1e-1)\n",
        "print(f'Opt array: {opt}')\n",
        "print(f'Time to compute: {time_to_compute}')\n",
        "print(f'losses: {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1nq1rasK2w"
      },
      "source": [
        "# Newton-step-style algorithm from [existing literature](https://arxiv.org/pdf/1602.04302v1.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IU7CO7TAgEJR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final loss: [DeviceArray(8256., dtype=float64), DeviceArray(5542.63719423, dtype=float64), DeviceArray(2847.64931587, dtype=float64), DeviceArray(1534.60983352, dtype=float64), DeviceArray(1386.83297443, dtype=float64), DeviceArray(943.14572093, dtype=float64), DeviceArray(868.39020584, dtype=float64), DeviceArray(857.89685045, dtype=float64), DeviceArray(847.14937673, dtype=float64), DeviceArray(843.53326629, dtype=float64), DeviceArray(835.35122893, dtype=float64), DeviceArray(828.55243871, dtype=float64), DeviceArray(777.4971529, dtype=float64), DeviceArray(773.9535248, dtype=float64), DeviceArray(761.10506948, dtype=float64), DeviceArray(752.1550407, dtype=float64), DeviceArray(749.76137602, dtype=float64), DeviceArray(747.1469614, dtype=float64), DeviceArray(745.14661879, dtype=float64), DeviceArray(738.15446089, dtype=float64), DeviceArray(734.79461947, dtype=float64), DeviceArray(730.44398639, dtype=float64), DeviceArray(728.67582355, dtype=float64), DeviceArray(727.45791161, dtype=float64), DeviceArray(722.30818594, dtype=float64), DeviceArray(718.98782356, dtype=float64), DeviceArray(717.7370258, dtype=float64), DeviceArray(716.91753463, dtype=float64), DeviceArray(716.19019102, dtype=float64), DeviceArray(715.52257545, dtype=float64), DeviceArray(714.87651001, dtype=float64), DeviceArray(710.85843437, dtype=float64), DeviceArray(710.15637335, dtype=float64), DeviceArray(708.99834523, dtype=float64), DeviceArray(708.22806403, dtype=float64), DeviceArray(707.66526858, dtype=float64), DeviceArray(707.21270433, dtype=float64), DeviceArray(706.77947556, dtype=float64), DeviceArray(703.96779652, dtype=float64), DeviceArray(703.52036988, dtype=float64), DeviceArray(703.25610141, dtype=float64), DeviceArray(702.6754904, dtype=float64), DeviceArray(702.30783803, dtype=float64), DeviceArray(701.86915457, dtype=float64), DeviceArray(701.54647683, dtype=float64), DeviceArray(701.21980433, dtype=float64), DeviceArray(700.92183207, dtype=float64), DeviceArray(700.62444148, dtype=float64), DeviceArray(700.34075085, dtype=float64), DeviceArray(700.06028452, dtype=float64), DeviceArray(699.78889729, dtype=float64), DeviceArray(699.52170912, dtype=float64), DeviceArray(699.26162418, dtype=float64), DeviceArray(699.0060443, dtype=float64), DeviceArray(698.14602563, dtype=float64), DeviceArray(697.53548455, dtype=float64), DeviceArray(697.22072872, dtype=float64), DeviceArray(697.01692738, dtype=float64), DeviceArray(696.56023439, dtype=float64), DeviceArray(696.47332258, dtype=float64), DeviceArray(696.24040282, dtype=float64), DeviceArray(696.00804411, dtype=float64), DeviceArray(695.82010696, dtype=float64), DeviceArray(695.61626271, dtype=float64), DeviceArray(695.43673899, dtype=float64), DeviceArray(695.24639371, dtype=float64), DeviceArray(695.0736545, dtype=float64), DeviceArray(694.89299548, dtype=float64), DeviceArray(694.72665954, dtype=float64), DeviceArray(694.5539983, dtype=float64), DeviceArray(694.39385386, dtype=float64), DeviceArray(694.22821749, dtype=float64), DeviceArray(694.07409754, dtype=float64), DeviceArray(693.91482359, dtype=float64), DeviceArray(693.76658873, dtype=float64), DeviceArray(693.61316992, dtype=float64), DeviceArray(693.47071007, dtype=float64), DeviceArray(693.32272244, dtype=float64), DeviceArray(693.18596303, dtype=float64), DeviceArray(693.04302661, dtype=float64), DeviceArray(692.91193648, dtype=float64), DeviceArray(692.77368977, dtype=float64), DeviceArray(692.64828995, dtype=float64), DeviceArray(692.51436983, dtype=float64), DeviceArray(692.39474269, dtype=float64), DeviceArray(692.26476573, dtype=float64), DeviceArray(692.15106324, dtype=float64), DeviceArray(692.02460673, dtype=float64), DeviceArray(691.91705554, dtype=float64), DeviceArray(691.79363807, dtype=float64), DeviceArray(691.69253796, dtype=float64), DeviceArray(691.57160153, dtype=float64), DeviceArray(691.47731255, dtype=float64), DeviceArray(691.35821097, dtype=float64), DeviceArray(691.3226721, dtype=float64), DeviceArray(691.22419569, dtype=float64), DeviceArray(691.11502151, dtype=float64), DeviceArray(691.02496456, dtype=float64), DeviceArray(690.91761241, dtype=float64), DeviceArray(690.88646855, dtype=float64)]\n",
            "Time in loop: [0.0, 0.11574053764343262, 0.17793679237365723, 0.24112462997436523, 0.30191993713378906, 0.3877236843109131, 0.47393321990966797, 0.5590667724609375, 0.642885684967041, 0.7058076858520508, 0.7714540958404541, 0.8417887687683105, 0.9022238254547119, 0.964667797088623, 1.050719976425171, 1.135894775390625, 1.1971726417541504, 1.2590687274932861, 1.3221168518066406, 1.4087069034576416, 1.474855661392212, 1.5361943244934082, 1.597740650177002, 1.6588597297668457, 1.7201025485992432, 1.8061962127685547, 1.868941068649292, 1.9293479919433594, 2.016683340072632, 2.1036717891693115, 2.1654117107391357, 2.250404119491577, 2.3116586208343506, 2.373885154724121, 2.435202121734619, 2.5214827060699463, 2.606245756149292, 2.6679446697235107, 2.7527945041656494, 2.840092182159424, 2.902935028076172, 2.963555097579956, 3.0255982875823975, 3.087661027908325, 3.151806116104126, 3.2396273612976074, 3.3004486560821533, 3.3862802982330322, 3.44810152053833, 3.535276412963867, 3.6215643882751465, 3.7086002826690674, 3.794203996658325, 3.855008125305176, 3.9410924911499023, 4.0274817943573, 4.113263368606567, 4.175664663314819, 4.2616705894470215, 4.322834730148315, 4.384410381317139, 4.459614515304565, 4.541309118270874, 4.628064870834351, 4.708094835281372, 4.831433534622192, 4.896674871444702, 4.980217933654785, 5.041230201721191, 5.126678943634033, 5.186406850814819, 5.271474838256836, 5.332365036010742, 5.41628885269165, 5.477278232574463, 5.56490683555603, 5.626342535018921, 5.733735799789429, 5.796860456466675, 5.906283617019653, 5.967560291290283, 6.0512375831604, 6.112395763397217, 6.196965456008911, 6.258624315261841, 6.321429967880249, 6.38336181640625, 6.445860147476196, 6.506106615066528, 6.566488742828369, 6.627748727798462, 6.714541673660278, 6.792069911956787, 6.903399467468262, 7.015044927597046, 7.098664283752441, 7.158961057662964, 7.220024585723877, 7.306428670883179, 7.3919806480407715]\n",
            "Min eval: 0.15775935177223585\n"
          ]
        }
      ],
      "source": [
        "# Implementing Alg 1 from https://arxiv.org/pdf/1602.04302v1.pdf.\n",
        "\n",
        "def compute_newton_direction(Z, grad, max_iter: int = 5):\n",
        "  \"\"\"Implements algorithm 2 from the referenced paper.\"\"\"\n",
        "  # Initialize according to line 4.\n",
        "  D = jnp.zeros_like(grad)\n",
        "  R = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "  # Set diag of D and R to zero; line 5\n",
        "  diag_elements = jnp.diag_indices_from(D)\n",
        "  D = D.at[diag_elements].set(0)\n",
        "  R = R.at[diag_elements].set(0)\n",
        "  # Initialize P and r_old as in line 6.\n",
        "  P = R\n",
        "  # Interestingly, this is the inner product used in the paper.\n",
        "  r_old = jnp.sum(R * R)\n",
        "  for i in range(max_iter):\n",
        "    # Set B and alpha as in line 8\n",
        "    B = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "    alpha = r_old / jnp.sum(P * B)\n",
        "    # Update D and R as in line 9\n",
        "    D = D + alpha * P\n",
        "    R = R - alpha * B\n",
        "    # Set diags of D anr R to 0, as in line 10\n",
        "    D = D.at[diag_elements].set(0)\n",
        "    R = R.at[diag_elements].set(0)\n",
        "    # Set r_new and update P, as in line 11\n",
        "    r_new = jnp.sum(R * R)\n",
        "    P = R + r_new / r_old * P\n",
        "    # Update r_old; line 12\n",
        "    r_old = r_new\n",
        "    if jnp.max(jnp.abs(R)) == 0:\n",
        "      # Everything nans if this is violated. I assume this loop should terminate in this case.\n",
        "      break\n",
        "  return D\n",
        "\n",
        "def optimize_factorization_newton_step(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, init_lr=1.):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Setting to 1 reproduces the paper of interest; see section 4.2.\n",
        "  # We parameterize for the purposes of tuning.\n",
        "  lr = init_lr\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr):\n",
        "    \"\"\"Computes step size as in Sec 4.2 of referenced paper.\"\"\"\n",
        "    candidate = x_iter + newton_dir * init_lr\n",
        "    # This is essentially the method for checking positive-definiteness proposed\n",
        "    # by the paper.\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "    # Equation (16)\n",
        "    target_decrease = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad * newton_dir)\n",
        "    if compiled_loss(candidate) <= target_decrease:\n",
        "      return candidate\n",
        "    return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compiled_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    inv_x = jnp.linalg.inv(x_iter)\n",
        "    newton_direction = compute_newton_direction(Z=inv_x, grad=grad)\n",
        "    loss_array.append(loss)\n",
        "    x_iter = find_next_iterate_armijo(x_iter, grad, newton_direction, lr)\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2 \n",
        "    time_array.append(time.time() - start)\n",
        "\n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "opt, losses, time_in_loop = optimize_factorization_newton_step(s_matrix, 100, jnp.eye(s_matrix.shape[0]))\n",
        "print(f'Final loss: {losses}')\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(f'Min eval: {jnp.min(jnp.linalg.eigh(opt)[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dseTAZa-Dm9t"
      },
      "source": [
        "# Data and plot generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ekxkv8DDMKqq"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m _MAX_ITERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmatrix_factorization_speed_data\u001b[39m(matrix, dim: \u001b[38;5;28mint\u001b[39m, max_inv_rtol: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, max_iter: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m   \u001b[38;5;66;03m# We fix a seed for reproducability, though the results are generally uniform\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# in this seed.\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "_MAX_ITERS = 1000\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def matrix_factorization_speed_data(matrix, dim: int, max_inv_rtol: int = 20, max_iter: int = 1000):\n",
        "\n",
        "  # We fix a seed for reproducability, though the results are generally uniform\n",
        "  # in this seed.\n",
        "  key = jax.random.PRNGKey(256)\n",
        "  initial_v = jax.random.uniform(key=key, shape=jnp.diag(matrix).shape)\n",
        "\n",
        "  fp_data = {'x': [], 'y': []}\n",
        "  gd_data = {'x': [], 'y': []}\n",
        "  ns_data = {'x': [], 'y': []}\n",
        "\n",
        "  _, losses_fp, time_to_compute_fp = optimize_factorization_fixed_point(matrix, _MAX_ITERS, rtol=10**-max_inv_rtol, initial_v=initial_v)\n",
        "  fp_data['x'] = time_to_compute_fp\n",
        "  fp_data['y'] = [float(x.to_py()) for x in losses_fp]\n",
        "\n",
        "  initial_x = compute_normalized_x_from_vector(matrix, initial_v)\n",
        "\n",
        "  n_iters = max_iter\n",
        "  # True gradient descent on the convex problem\n",
        "  _, losses, time_in_loop = optimize_factorization_grad_descent(matrix, n_iters, initial_x, lr=1., use_armijo_rule=True)\n",
        "  gd_data['x'] = time_in_loop\n",
        "  gd_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  # The Newton-direction-based method of https://arxiv.org/pdf/1602.04302v1.pdf\n",
        "  _, losses, time_in_loop = optimize_factorization_newton_step(matrix, n_iters, initial_x, init_lr=1.)\n",
        "  ns_data['x'] = time_in_loop\n",
        "  ns_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  df = pd.DataFrame({'Elapsed Time (s)': fp_data['x'] + gd_data['x'] + ns_data['x'], \n",
        "                   'Loss': fp_data['y'] + gd_data['y'] + ns_data['y'],\n",
        "                   'Method': ['Fixed point'] * len(fp_data['x']) + ['Gradient descent'] * len(gd_data['x']) + ['Newton-based step'] * len(ns_data['x'])})\n",
        "  return df\n",
        "\n",
        "\n",
        "def prefix_sum_factorization_speed_data(dim: int, max_inv_rtol: int = 20, max_iter: int = 1000) -> pd.DataFrame:\n",
        "  s_matrix = jnp.tril(jnp.ones(shape=(dim, dim)))\n",
        "  return matrix_factorization_speed_data(s_matrix, dim, max_inv_rtol, max_iter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QzVm0QGVSa6g"
      },
      "outputs": [],
      "source": [
        "def generate_and_plot_data(dim: int, max_inv_rtol: int, max_iter: int):\n",
        "\n",
        "  df = prefix_sum_factorization_speed_data(dim=dim, max_inv_rtol=max_inv_rtol, max_iter=max_iter)\n",
        "  palette = sns.color_palette('muted')\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  sns.set_style('whitegrid')\n",
        "  sns.set_context('paper')\n",
        "  line = sns.lineplot(data=df,\n",
        "              x='Elapsed Time (s)',\n",
        "              y='Loss',\n",
        "              hue='Method',\n",
        "              palette=[palette[0], palette[1], palette[2]],\n",
        "              )\n",
        "\n",
        "  # Compute the max time that all methods generated data for.\n",
        "  max_elapsed_times = []\n",
        "  for method in df['Method'].unique():\n",
        "    max_elapsed_times.append(np.max(df[df['Method'] == method]['Elapsed Time (s)']))\n",
        "  max_all_elapsed_time = max(max_elapsed_times)\n",
        "\n",
        "  max_loss = np.max(df['Loss'])\n",
        "  min_loss = np.min(df['Loss'])\n",
        "\n",
        "  # Heuristic method to set the ranges for visibility\n",
        "  plt.ylim(min_loss - (max_loss - min_loss) * 0.05, max_loss)\n",
        "  plt.xlim(0, max_all_elapsed_time)\n",
        "  return df \n",
        "\n",
        "# df = generate_and_plot_data(2048, 10, 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Prefix-sum matrix factorization optimization.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1eY_VCuTGPrvwhGEH8f3QKNzAtAw-BkPC",
          "timestamp": 1653448381403
        },
        {
          "file_id": "1Wfhb9XY3uGWfA8jm2gYdx28JhODLwcho",
          "timestamp": 1652721590159
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python (dp-matfac-env)",
      "language": "python",
      "name": "dp-matfac-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
